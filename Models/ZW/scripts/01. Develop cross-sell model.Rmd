---
title: "Develop cross-sell model"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# PACKAGES

```{r message=FALSE, warning=FALSE}
rm(list = ls())

library(doParallel)
library(arrow)
library(sparklyr)
library(dtplyr)
library(recommenderlab)
library(DBI)
library(RSQLite)
library(tidyverse)
```


# VALIDATION

## Evaluation scheme


```{r}
products_rating_matrix <-
readRDS(file = "../../../Data Prep/ZW/02. Create rating matrices/outputs/products_rating_matrix.rds")

dim(products_rating_matrix)
```

Train-test split. Since most customers hold just one product, a cross-validated training cannot be done.

```{r}
summary(rowCounts(products_rating_matrix))
```

```{r}
es <- 
evaluationScheme(data = products_rating_matrix, 
                 method = "split",
                 train = 0.7,
                 k = 1,
                 given = 1)
```

## List of recommenders

```{r}
recommenderRegistry$get_entry_names()
```

AR removed as it does not provide meaningful results despite reducing confidence to 0.0

```{r}
algorithms <- list(
  popular = list(name = "POPULAR", param = NULL),
  ibcf = list(name = "IBCF", param = NULL),
  ubcf = list(name = "UBCF", param = NULL),
  random = list(name = "RANDOM", param = NULL),
  rerecommend = list(name = "RERECOMMEND", param = NULL),
  ar = list(name = "AR", param = list(confidence = 0.0)))
```

## Obtain evaluation results

```{r}
gc()

doParallel::registerDoParallel(parallel::detectCores())

ev_list <- 
evaluate(x = es,
         method = algorithms,
         type = "topNList",
         n = seq(1,10,1))

doParallel::stopImplicitCluster()

gc()
```

# EVALUATION METRICS

## Definition

Reference: [Precision-Recall formula](https://en.wikipedia.org/wiki/Precision_and_recall) in the Information Retrieval context.

Precision: Out of all the recommendations returned by the tool, how many of those were relevant? Having a high number here means that many of the products recommended were relevant to the customer. This can be viewed as a measure of false-positives.

Recall: Out of all the recommendations returned by the tool, were all relevant recommendation in that list or some were missing? Having a high number here means that at least one of those products being recommended was relevant.

## PR Curves

```{r}
names(ev_list)
```


```{r}
plot(x = ev_list, y = "prec/rec", annotate=TRUE, legend="topleft")
```

## Metrics

```{r}
avg(ev_list$ibcf) %>% 
  as_tibble() %>% 
  select(precision,recall,FPR,TPR,n) %>% 
  filter(n %in% c(1,2,3))
```

# MODEL TRAINING

## Train actual model

Notes:
A hybrid recommender will be used since an IBCF recommender on its own does not generate ratings across all business lines.

The hybrid recommender could not be evaluated using the evaluation scheme as the process was taking an extended period of time.

```{r}
zw_ibcf = Recommender(data = products_rating_matrix, method = "IBCF")
zw_pop = Recommender(data = products_rating_matrix, method = "POPULAR")
zw_hyb <- HybridRecommender(zw_ibcf, zw_pop, weights = c(0.9,0.1))


zw_rec_model <- zw_hyb
```


## Extract ratings

### Approach 1

Preferred approach.

```{r eval=FALSE, include=FALSE}
# Chunk taking too long

zw_recommendations <- 
  predict(object = zw_rec_model, 
          newdata = products_rating_matrix, 
          type = "ratings")

```

### Approach 2

Using matrices.

```{r}
zw_ibcf_mat <- 
  predict(object = zw_ibcf,
          newdata = products_rating_matrix,
          type = "ratings") %>% 
  as("matrix")
```


```{r eval=FALSE, include=FALSE}
# Getting a memory error: Run on Google Colab
zw_pop_mat <- 
  predict(object = zw_pop,
          newdata = products_rating_matrix,
          type = "ratings") %>% 
  as("matrix")
```


```{r}
zw_pop_mat <- readRDS(file = "../outputs/zw_pop_mat.rds")

dim(zw_pop_mat)
```


Replace NAs with zero and obtain hybrid matrix.

```{r}
zw_pop_mat[is.na(zw_pop_mat)]=0
zw_ibcf_mat[is.na(zw_ibcf_mat)]=0


zw_hyb_mat = (zw_ibcf_mat*0.9) + (zw_pop_mat*0.1)
```

Obtain first 5 product recommendations of first 2 customers.

```{r}
zw_hyb_mat[1:2,1:5] %>% 
  as.data.frame() %>%
  rownames_to_column("user") %>% 
  pivot_longer(cols = -user,
               names_to = "item",
               values_to = "rating") %>% 
  filter(rating > 0)
```

Comparison of results using `predict()`

```{r}
predict(object = zw_hyb, 
        newdata = products_rating_matrix[1:2,], 
        type = "ratings") %>% 
  as("data.frame") %>% 
  filter(item %in% colnames(products_rating_matrix)[1:5])
```

### Ratings

```{r}
gc()

zw_recommendations <-
zw_hyb_mat %>% 
  as.data.frame() %>% 
  rownames_to_column("UNIQUE_ID") %>%
  pivot_longer(cols = -UNIQUE_ID,
               names_to = "PRODUCT",
               values_to = "rating") %>% 
  filter(rating > 0)

zw_recommendations
```

## Add business lines

Add product business lines to ratings.

```{r}
product_businesslines <-
readRDS(file = "../../../Data Prep/ZW/03. Extract additional customer details/outputs/product_businesslines.rds")

product_businesslines

dim(products_rating_matrix)
```

```{r}
zw_recommendations_proc <-
zw_recommendations %>%
  left_join(product_businesslines, by = "PRODUCT")

zw_recommendations_proc
```

```{r eval=FALSE, include=FALSE}
gc()

# copy_to(dest = sc, df = zw_recommendations_proc, overwrite = TRUE, temporary = FALSE, memory = FALSE)

write_parquet(x = zw_recommendations_proc, sink = "../outputs/zw_recommendations_proc.parquet")

```

## Add max rating, arrange

* Add maximum product rating per customer
* Sort by max rating and rating per customer
* Group data by customer and business line
* Limit to top 5 recommendations per customer per business line


```{r eval=FALSE, include=FALSE}
zw_recommendations_df <-
zw_recommendations_proc %>%
  group_by(UNIQUE_ID) %>% 
  mutate(max_rating = max(rating)) %>% 
  arrange(desc(max_rating),
          desc(rating), .by_group = TRUE) %>% 
  ungroup() %>%
  group_by(UNIQUE_ID, BUSINESS_LINE) %>% 
  slice(1:5)
```

Replicate the above in SQL.

Because `slice()` is not implemented yet, well design a workaround in SQL using `RANK()`:
* Group the dataframe as required
* Add a rank column
* Filter top 5 by rank

### Approach 1

Create data prep SQLite database. The problem with this is that it creates a very huge database of 7 GB compared with the same parquet file of <300 MB. Also, it gives a disk full error.

```{r}
zw_dataprep_db <- "../outputs/zw_dataprep.db"
con_prep <- dbConnect(drv = RSQLite::SQLite(), zw_dataprep_db)
```

Chunk and pull into an SQLite database

```{r}
zw_recs_proc_path <- "../outputs/parquets/zw_recommendations_proc.parquet"

pq <- ParquetFileReader$create(file = zw_recs_proc_path)

pq$GetSchema()
pq$num_row_groups
```


```{r}
doParallel::registerDoParallel(cores = 2)
zw_recs_proc_path <- "../outputs/parquets/zw_recommendations_proc.parquet"
zw_recommendations_proc <- read_parquet(file = zw_recs_proc_path)
doParallel::stopImplicitCluster()
```


```{r}
doParallel::registerDoParallel(cores = 2)
copy_to(dest = con_prep, df = zw_recommendations_proc, name = "zw_recommendations_proc", overwrite = TRUE, temporary = FALSE)
doParallel::stopImplicitCluster()
```


```{r}
dbListTables(conn = con_prep)

rm(zw_recommendations_proc)
```


```{r}
zw_recommendations_df_query <-
tbl(src = con_prep, "zw_recommendations_proc") %>% 
  head(1000) %>% 
  group_by(UNIQUE_ID) %>% 
  mutate(max_rating = max(rating)) %>% 
  arrange(desc(max_rating),
          desc(rating), .by_group = TRUE) %>% 
  ungroup() %>%
  group_by(UNIQUE_ID, BUSINESS_LINE) %>%
  mutate(rank = row_number()) %>% 
  filter(rank <= 5)
```


```{r}
doParallel::registerDoParallel(cores = 2)
copy_to(dest = con_prep, df = zw_recommendations_df_query, name = "zw_recommendations_df", overwrite = TRUE, temporary = FALSE)
doParallel::stopImplicitCluster()
```

### Approach 2

Using spark and saving the results to a parquet file. Out of memory issues.

```{r}
spark_installed_versions()
```

Set configurations. Useful links for avoiding out of memory errors: 
* [Config options](https://spark.rstudio.com/deployment/#collect)
* [Default settings](https://spark.apache.org/docs/latest/configuration.html)
* [A working config](https://stackoverflow.com/questions/41384336/running-out-of-heap-space-in-sparklyr-but-have-plenty-of-memory)

```{r}
# This retrieves default spark settings and allows us to update with our own
Sys.setenv("SPARK_MEM" = "500g")

config <- spark_config()
config$spark.driver.maxResultSize <- "500g"
config$spark.driver.memory <- "500g"
config$spark.executor.memory <- "500g"
config$`sparklyr.shell.driver-memory` <- "8g"
```

Configurations

```{r}
spark_config()

config


class(spark_config())

class(config)

#config::get()
```

Create spark connection. Reconnect to load new settings and confirm them.

```{r}
spark_disconnect(sc = sc)

sc <- spark_connect(master = "local", config = config)

sc
```

Read parquet file into spark database. Use parallel processing until the end since we'll be working with long operations.

```{r}
registerDoParallel(cores = 2)

spark_read_parquet(sc = sc, name = "zw_recommendations_proc", path = "../outputs/parquets/zw_recommendations_proc.parquet")

dbListTables(conn = sc)
```

Prepare SQL query.

```{r}
zw_recommendations_df_query <-
tbl(src = sc, "zw_recommendations_proc") %>% 
  group_by(UNIQUE_ID) %>% 
  mutate(max_rating = max(rating)) %>% 
  arrange(desc(max_rating),
          desc(rating), .by_group = TRUE) %>% 
  ungroup() %>%
  group_by(UNIQUE_ID, BUSINESS_LINE) %>%
  mutate(rank = row_number()) %>% 
  filter(rank <= 5)

class(zw_recommendations_df_query)
```

Using compute to create the table remotely

```{r}
compute(x = zw_recommendations_df_query, name = "zw_recommendations_df", overwrite = TRUE)

dbListTables(conn = sc)
```
Execute SQL query. Collect fails collecting the table into memory regardless of the settings.

```{r eval=FALSE, include=FALSE}
gc()

rm(zw_recommendations_df)

zw_recommendations_df <- zw_recommendations_df_query %>% 
  collect()

class(zw_recommendations_df)
dim(zw_recommendations_df)
```

Collect vectors into memory works.

```{r}
df_BUSINESS_LINE <- 
  tbl(src = sc, "zw_recommendations_df") %>% 
  select(BUSINESS_LINE) %>% 
  collect()
```


```{r}
saveRDS(object = df_UNIQUE_ID, file = "../outputs/df_UNIQUE.rds")
```


### Approach 3

Analyse data in Postgres database. Queries taking too long to execute.

```{r}
con_prep <- dbConnect(drv = RPostgres::Postgres(), user = "postgres", password = "john", dbname = "zw_dataprep")

dbListTables(conn = con_prep)
```

Create table in Postgres.

```{r}
# gc()
# 
# registerDoParallel(cores = 2)
# 
# copy_to(dest = con_prep, df = zw_recommendations_proc, name = "zw_recommendations_proc", overwrite = TRUE, temporary = FALSE)
# 
# dbListTables(conn = con_prep)
# 
# stopImplicitCluster()
```


```{r}
zw_recommendations_df_query <-
tbl(src = con_prep, "zw_recommendations_proc") %>% 
  group_by(UNIQUE_ID) %>% 
  mutate(max_rating = max(rating)) %>%
  arrange(desc(max_rating),
          desc(rating), .by_group = TRUE) %>% 
  ungroup() %>%
  group_by(UNIQUE_ID, BUSINESS_LINE) %>%
  mutate(rank = row_number()) %>% 
  filter(rank <= 5)

class(zw_recommendations_df_query)

zw_recommendations_df_query %>% 
  show_query()
```

Execute SQL to create table remotely.

```{r}
registerDoParallel(cores = 2)

compute(x = zw_recommendations_df_query, name = "zw_recommendations_df", temporary = FALSE, overwrite = TRUE) %>% 
  show_query()

dbListTables(conn = con_prep)

stopImplicitCluster()
```


### Approach 4

Apache Spark with Postgres backend. This has the speed of Apache and persistency of Postgres. Perform all operations on lazy tables and only copy final results to Postgres.

```{r}
spark_installed_versions()
```



```{r}
config <- spark_config()
config$`sparklyr.shell.driver-class-path` <- "E:\\Softwares\\postgresql-42.2.22.jar"
```


```{r}
sc <- spark_connect(master = "local", config = config)

sc
```


Read parquet into Hadoop.

```{r}
gc()

registerDoParallel(cores = 2)

zw_recommendations_proc <- spark_read_parquet(sc = sc, path = "../outputs/parquets/zw_recommendations_proc.parquet")

class(zw_recommendations_proc)

stopImplicitCluster()
```

Perform SQL operations on the lazy table.

```{r}
zw_recommendations_df_query <-
  zw_recommendations_proc %>% 
  group_by(UNIQUE_ID) %>% 
  mutate(max_rating = max(rating)) %>% 
  arrange(desc(max_rating),
          desc(rating), .by_group = TRUE) %>% 
  ungroup() %>%
  group_by(UNIQUE_ID, BUSINESS_LINE) %>%
  mutate(rank = row_number()) %>% 
  filter(rank <= 5)

class(zw_recommendations_df_query)
```

* Perform all operations on lazy tables and only copy the final results to Postgres.

### CONTINUE



Write table from Hadoop into Postgres.

Useful links:
* https://github.com/sparklyr/sparklyr/issues/1067
* https://therinspark.com/data.html#jdbc

```{r}
gc()

registerDoParallel(cores = 2)

spark_write_jdbc(x = zw_recommendations_df_query, 
                 name = "zw_recommendations_df", 
                 mode = "overwrite",
                 options = list(url = "jdbc:postgresql://localhost:5432/zw_dataprep",
                                user = "postgres",
                                password = "john"))

stopImplicitCluster()
```

Read table from Postgres to Hadoop.

```{r}
spark_read_jdbc(sc = sc, 
                name = "test1", 
                options = list(url = "jdbc:postgresql://localhost:5432/zw_dataprep",
                               user = "postgres",
                               password = "john",
                               dbtable = "test2"))
```








## Add product values

```{r}
product_values <-
readRDS(file = "../../../Data Prep/ZW/03. Extract additional customer details/outputs/product_values.rds")

product_values
```


```{r eval=FALSE, include=FALSE}
zw_recommendations_df_proc <-
zw_recommendations_df %>% 
  left_join(product_values, by = "PRODUCT") %>% 
  select(-accounts)

zw_recommendations_df_proc
```


Replicate the above operation in spark

```{r}
spark_read_parquet(sc = sc, name = "zw_recommendations_df", path = "../outputs/parquets/zw_recommendations_df.parquet")

dbListTables(conn = sc)

zw_recommendations_df_proc <-
tbl(src = sc, "zw_recommendations_df") %>% 
  left_join(product_values, by = "PRODUCT") %>% 
  select(-accounts)

zw_recommendations_df_proc
```


## Add customer details & grouping

Merge ratings with customer details.

```{r}
customer_details_final <-
readRDS(file = "../../../Data Prep/ZW/03. Extract additional customer details/outputs/customer_details_final.rds")

customer_details_final
```


```{r}
table_order <- c("UNIQUE_ID", "PRODUCT", "BUSINESS_LINE", "rating", "max_rating", "ownership", "intermediated", "product_value", "customer_details")

zw_recommendations_unique_id <-
zw_recommendations_df_proc %>% 
  left_join(customer_details_final, by = "UNIQUE_ID") %>% 
  relocate(BUSINESS_LINE, .after = PRODUCT) %>% 
  group_by(UNIQUE_ID, BUSINESS_LINE) %>% 
  select(all_of(table_order))

zw_recommendations_unique_id
```


## Add Accounts

Add all Account Numbers associated with each Unique ID

```{r}
holding_paths <-
dir(path = "../../../Data Prep/ZW/01. Determine product holding/outputs/objects/",
    pattern = "holding",
    all.files = FALSE, 
    full.names = TRUE, 
    ignore.case = TRUE)

uniqueid_accounts <-
holding_paths %>% 
  map_dfr(.f = readRDS) %>% 
  select(UNIQUE_ID, ACCOUNT_NO) %>% 
  distinct(UNIQUE_ID, .keep_all = TRUE)

uniqueid_accounts
```


```{r}
zw_recommendations_detailed <-
zw_recommendations_unique_id %>% 
  left_join(uniqueid_accounts, by = "UNIQUE_ID") %>%
  relocate(ACCOUNT_NO, .after = UNIQUE_ID) %>% 
  ungroup() %>%
  group_by(ACCOUNT_NO, BUSINESS_LINE) %>% 
  select(-UNIQUE_ID)

zw_recommendations_detailed
```

## Sense check

Customers with National IDs having multiple Account Numbers.

```{r}
uniqueid_accounts %>% 
  filter(str_detect(string = ACCOUNT_NO, pattern = ","))
```

Confirm the above results.

```{r}
zw_recommendations_detailed %>% 
  ungroup() %>% 
  distinct(ACCOUNT_NO) %>% 
  filter(str_detect(string = ACCOUNT_NO, pattern = ","))
```



# CREATE DATABASE

Create sqlite3 database to hold and query records. One country, one database, many tables.

Ref: https://www.youtube.com/watch?v=wXEZZ2JT3-k

## Database Connection

```{r}
zw_sqlite_path <- "../../../App/uap_cross_sell/db/zw_cs.db"
con_zw <- dbConnect(drv = SQLite(), zw_sqlite_path)
```


```{r}
dbListTables(conn = con_zw)
```

## Database tables

Create database tables of required data.

### Detailed recommendations

```{r}
copy_to(dest = con_zw, 
        df = zw_recommendations_detailed, 
        name = "recommendations_detailed",
        overwrite = TRUE, 
        temporary = FALSE)
```

Compare the two tables...

```{r}
dim(zw_recommendations_detailed)

tbl(src = con_zw, "recommendations_detailed") %>% 
  collect() %>% 
  dim()
```


### Product business lines

```{r}
copy_to(dest = con_zw, 
        df = product_businesslines, 
        name = "product_businesslines", 
        overwrite = TRUE, 
        temporary = FALSE)
```


### Product values

```{r}
copy_to(dest = con_zw, 
        df = product_values, 
        name = "product_values", 
        overwrite = TRUE,
        temporary = FALSE)
```


## Confirm tables

```{r}
dbListTables(conn = con_zw)
```


```{r}
tbl(src = con_zw, "recommendations_detailed")
```


```{r eval=FALSE, include=FALSE}
# Not required
q <- "UPDATE product_values SET product_value = 99999, accounts = 99999"

DBI::dbSendQuery(conn = con_zw, statement = q)
```


# PREDICTIONS

## Scenario 1

Where you have a customer account number and want to 3 products per business line.

We select a customer whose unique identifier is:
* Account Number: 981741
* Hashed National ID: c11be68eb394fac45c32076299d16a386b771318675d348abcf8d76866f97866
* Multiple Account Numbers: f09b93a542666d06d857fab8ccc0e2a7ca8697c1b4ae706aa0e717cd49383a66

```{r}
#sample_cust_num <- paste0(rownames(products_rating_matrix)[1:2], collapse =",")
#records <- strsplit(x = sample_cust_num, split = ",")[[1]]

unique_ids <- c("981741",
                "c11be68eb394fac45c32076299d16a386b771318675d348abcf8d76866f97866",
                "f09b93a542666d06d857fab8ccc0e2a7ca8697c1b4ae706aa0e717cd49383a66")
```

### Predict

```{r}
predict(object = zw_rec_model, 
        newdata = products_rating_matrix[unique_ids,], 
        type = "ratings") %>% 
  as("data.frame") %>% 
  left_join(product_businesslines, by = c("item"="PRODUCT")) %>% 
  group_by(user, BUSINESS_LINE) %>% 
  arrange(desc(rating)) %>% 
  slice(1:3)
```

### Dataframe

Test cases
* A customer with multiple accounts ["10039100,30022583,41398995,30105335"].
* Two customers with multiple accounts ["41505354,41505340,41505336].
* A customer with multiple accounts and another without multiple accounts ["981741"].

#### Approach 1

```{r}
# Too slow
zw_recommendations_detailed %>% 
  filter(str_detect(string = ACCOUNT_NO, pattern = "30105335")) %>% 
  group_by(ACCOUNT_NO, BUSINESS_LINE) %>% 
  slice(1:3)
```

#### Approach 2

```{r}
convert_nums_to_pattern <- function(account_num){
  
  account_num_vec <- strsplit(x = account_num, split = ",")[[1]]
  pattern_account_num <- paste0(account_num_vec, collapse = "|")
  return(pattern_account_num)
  
}
```

Test the function.

```{r}
input_account_num <- str_remove_all(string = "30105335,41505340,  981741",
                                     pattern = " ")

convert_nums_to_pattern(account_num = input_account_num)
```
Implement the function.

```{r}
# Significantly faster
input_account_num <- c("30105335,   41505340,981741")
input_account_num <- str_remove_all(string = input_account_num, 
                                    pattern = " ")

cust_unique_ids <-
uniqueid_accounts %>% 
  filter(str_detect(string = ACCOUNT_NO, 
                    pattern = convert_nums_to_pattern(
                      account_num = input_account_num)
                    )) %>% 
  pull(UNIQUE_ID)


predict(object = zw_rec_model, 
        newdata = products_rating_matrix[cust_unique_ids,], 
        type = "ratings") %>% 
  as("data.frame") %>% 
  rename(UNIQUE_ID = user, PRODUCT = item) %>% 
  left_join(product_businesslines, by = "PRODUCT") %>% 
  left_join(uniqueid_accounts, by = "UNIQUE_ID") %>% 
  select(any_of(c("ACCOUNT_NO",table_order))) %>% 
  select(-UNIQUE_ID) %>% 
  group_by(ACCOUNT_NO, BUSINESS_LINE) %>% 
  arrange(desc(rating)) %>% 
  slice(1:3)
```

### Database

Database replication of the above results.

Save `uniqueid_accounts` to database

```{r}
copy_to(dest = con_zw,
        df = uniqueid_accounts,
        name = "uniqueid_accounts",
        overwrite = TRUE,
        temporary = FALSE)
```

#### REGEX

Regex in SQL.

```{r message=FALSE, warning=FALSE}
# Requires loading a udf from a dll file.
# NOT USED
query_stmt <- "SELECT * FROM uniqueid_accounts WHERE ACCOUNT_NO REGEXP '30105335';"
query_res <- dbSendStatement(conn = con_zw, statement = query_stmt)
```

#### LIKE

Using LIKE operator.

```{r}
query_stmt <- "select * from uniqueid_accounts limit 6;"
query_result <- dbSendStatement(conn = con_zw, statement = query_stmt)

query_stmt1 <- "SELECT * FROM uniqueid_accounts WHERE ACCOUNT_NO like '%30105335%';"
query_stmt2 <- "SELECT * FROM uniqueid_accounts WHERE ACCOUNT_NO like '%41505340%';"
query_stmt3 <- "SELECT * FROM uniqueid_accounts WHERE ACCOUNT_NO like '%981741%';"

query_result1 <- dbSendStatement(conn = con_zw, statement = query_stmt1)
dbFetch(res = query_result1)
dbClearResult(res = query_result1)

query_result2 <- dbSendStatement(conn = con_zw, statement = query_stmt2)
dbFetch(res = query_result2)
dbClearResult(res = query_result2)

query_result3 <- dbSendStatement(conn = con_zw, statement = query_stmt3)
dbFetch(res = query_result3)
dbClearResult(res = query_result3)
```

Applying `glue`

```{r}
test_acc <- "30105335"

test_stmt <-
  paste0("SELECT * FROM uniqueid_accounts WHERE ACCOUNT_NO LIKE '%",test_acc,"%'") %>% 
  SQL()

test_res <- dbSendQuery(conn = con_zw, statement = test_stmt)
test_tbl <- dbFetch(test_res)
dbClearResult(res = test_res)
test_tbl
```

Adding a `for()` loop

```{r}
input_account_num <- c("30105335,   41505340,981741")
input_account_num <- str_remove_all(string = input_account_num, 
                                    pattern = " ")
input_account_num <- str_split(string = input_account_num, 
                               pattern = ",")[[1]]


res_tbl <- tibble()
  
for(account in seq_along(input_account_num)){
  
  query_stmt <-
  paste0("SELECT * FROM uniqueid_accounts WHERE ACCOUNT_NO LIKE '%",input_account_num[account],"%'") %>% 
  SQL()
  
  query_result <- 
  dbSendStatement(conn = con_zw, 
                  statement = query_stmt)
  
  res_tbl <- rbind(res_tbl, dbFetch(res = query_result))
  dbClearResult(res = query_result)
}

cust_unique_ids <- res_tbl %>% pull(UNIQUE_ID)

cust_unique_ids
```

#### Predict

```{r}
predict(object = zw_rec_model, 
        newdata = products_rating_matrix[cust_unique_ids,], 
        type = "ratings") %>% 
  as("data.frame") %>% 
  rename(UNIQUE_ID = user, PRODUCT = item) %>% 
  left_join(product_businesslines, by = "PRODUCT") %>% 
  left_join(uniqueid_accounts, by = "UNIQUE_ID") %>% 
  left_join(product_values, by = "PRODUCT") %>% 
  left_join(customer_details_final, by = "UNIQUE_ID") %>% 
  select(any_of(c("ACCOUNT_NO",table_order))) %>% 
  select(-UNIQUE_ID) %>% 
  group_by(ACCOUNT_NO, BUSINESS_LINE) %>% 
  mutate(max_rating = max(rating)) %>% 
  relocate(max_rating, .after = rating) %>% 
  arrange(desc(rating)) %>% 
  slice(1:3)
```


```{r}
copy_to(dest = con_zw, 
        df = customer_details_final, 
        name = "customer_details_final",
        overwrite = TRUE, 
        temporary = FALSE)
```


#### User defined function


```{r}
predict_on_accounts <- function(account_num, recomm_limit){
  
# Clean & vectorize input values
input_account_num <- str_remove_all(string = account_num, 
                                    pattern = " ")
input_account_num <- str_split(string = input_account_num, 
                               pattern = ",")[[1]]

# Retrieve unique identifiers
res_tbl <- tibble()
  
for(account in seq_along(input_account_num)){
  
  query_stmt <-
  paste0("SELECT * FROM uniqueid_accounts WHERE ACCOUNT_NO LIKE '%",input_account_num[account],"%'") %>% 
  SQL()
  
  query_result <- 
  dbSendStatement(conn = con_zw, 
                  statement = query_stmt)
  
  res_tbl <- rbind(res_tbl, dbFetch(res = query_result))
  dbClearResult(res = query_result)
}

cust_unique_ids <- res_tbl %>% pull(UNIQUE_ID)

# Predict & merge
user_account_recommendations <-
predict(object = zw_rec_model, 
        newdata = products_rating_matrix[cust_unique_ids,], 
        type = "ratings") %>% 
  as("data.frame") %>% 
  rename(UNIQUE_ID = user, PRODUCT = item) %>% 
  left_join(product_businesslines, by = "PRODUCT") %>% 
  left_join(uniqueid_accounts, by = "UNIQUE_ID") %>% 
  left_join(product_values, by = "PRODUCT") %>% 
  left_join(customer_details_final, by = "UNIQUE_ID") %>% 
  select(any_of(c("ACCOUNT_NO",table_order))) %>% 
  select(-UNIQUE_ID) %>% 
  group_by(ACCOUNT_NO, BUSINESS_LINE) %>% 
  mutate(max_rating = max(rating)) %>% 
  relocate(max_rating, .after = rating) %>% 
  arrange(desc(rating)) %>% 
  slice(1:recomm_limit)

return(user_account_recommendations)
}
```

Test the function.

```{r}
predict_on_accounts(account_num = "30105335,   41505340,981741",
                    recomm_limit = 3)
```


## Scenario 2

You want to sort all customers across the group according to who's most likely to make a purchase and return 3 recommendations per business line.

Due to performance challenges, only the top 1000 records will be displayed while remaining dataset can be availed as a link for downloading.

We allow the end-user get the top n recommended customers based on this criteria:
* Intermediated - Yes/No
* Ownership - business line that owns the customer
* Product value - a range
* Rank e.g. 1-10

### Dataframe

```{r}
zw_recommendations_detailed %>% 
  ungroup() %>% 
  count(ownership)

zw_recommendations_detailed %>% 
  ungroup() %>% 
  filter(intermediated=="No") %>% 
  count(ownership)
```

```{r}
zw_recommendations_detailed %>% 
  filter(intermediated=="No") %>% 
  filter(ownership=="Lending") %>% 
  filter(product_value>=50000) %>% 
  filter(product_value<=200000) %>% 
  slice(1:3)
 #head(1000)
```

### Database

```{r}
tbl(src = con_zw, "recommendations_detailed") %>%
  group_by(ACCOUNT_NO, BUSINESS_LINE) %>% 
  filter(intermediated=="No") %>% 
  filter(ownership=="Lending") %>% 
  filter(product_value >= 50000) %>% 
  filter(product_value <= 200000) %>% 
  collect() %>% 
  slice(1:3)
  #head(1000)
```


## Scenario 3

Include additional products on top of what one has purchased and return 3 recommendations per business line.

### Procedure

Capture user selections. One product from each business line.

```{r}
user_choices <-
product_businesslines %>%
  group_by(BUSINESS_LINE) %>%
  slice_sample(n = 1) %>%
  pull(PRODUCT)

user_choices
```

Identify missing product names

```{r}
not_user_choices <- colnames(products_rating_matrix)[!colnames(products_rating_matrix) %in% user_choices]

length(not_user_choices)
```

Convert to dataframe.

```{r}
choices <- tibble(choice = 
rep(x = c(1,0), 
    times = c(length(user_choices), 
              length(not_user_choices))
    ))

user_choices_df <-
choices %>% 
  mutate(user_choices = c(user_choices, not_user_choices))

user_choices_df
```


```{r}
user_choices_df <- 
pivot_wider(data = user_choices_df,
            names_from = user_choices, 
            values_from = choice)

user_choices_df
```

Convert to binary rating matrix.

```{r}
user_choices_mat <- as.matrix(user_choices_df)

colnames(user_choices_mat)[1:10]

user_choices_ratmat <- as(user_choices_mat,"binaryRatingMatrix")

class(user_choices_ratmat)
```

Recommend products.

```{r}
user_choice_recommendations <- 
  predict(object = zw_rec_model, 
          newdata = user_choices_ratmat, 
          type="ratings")
```


```{r}
as(user_choice_recommendations, "data.frame") %>% 
  rename(PRODUCT = item) %>% 
  left_join(product_businesslines, by = "PRODUCT") %>% 
  left_join(product_values, by = "PRODUCT") %>% 
  relocate(BUSINESS_LINE, .after = PRODUCT) %>% 
  group_by(BUSINESS_LINE) %>% 
  arrange(desc(rating), .by_group = TRUE) %>%
  select(-accounts) %>% 
  slice(1:3)
```

### User-defined function

Convert to a user function

```{r}
predict_on_choices <- function(user_choices, recomm_limit){
  
  #Products not selected
  not_user_choices <- colnames(products_rating_matrix)[!colnames(products_rating_matrix) %in% user_choices]
  
  #Convert to a table
  choices <- 
    tibble(choice = rep(x = c(1,0), 
                        times = c(length(user_choices), 
                                  length(not_user_choices))
    ))

user_choices_df <-
choices %>% 
  mutate(user_choices = c(user_choices, not_user_choices))
  
 #Convert to wide format
user_choices_df <- 
pivot_wider(data = user_choices_df,
            names_from = user_choices, 
            values_from = choice)

 #Convert to binaryRatingMatrix
user_choices_mat <- as.matrix(user_choices_df)
user_choices_ratmat <- as(user_choices_mat,"binaryRatingMatrix")

 #Predict using model
user_choice_recommendations <- 
  predict(object = zw_rec_model, 
          newdata = user_choices_ratmat, 
          type="ratings")

 #Convert prediction to dataframe
user_choice_recommendations_df <-
as(user_choice_recommendations, "data.frame") %>% 
  rename(PRODUCT = item) %>%
  left_join(product_businesslines, by = "PRODUCT") %>% 
  left_join(product_values, by = "PRODUCT") %>% 
  relocate(BUSINESS_LINE, .after = PRODUCT) %>% 
  group_by(BUSINESS_LINE) %>% 
  arrange(desc(rating), .by_group = TRUE) %>% 
  slice(1:recomm_limit) %>% 
  select(-accounts)

return(user_choice_recommendations_df)
}
```


```{r}
predict_on_choices(user_choices = user_choices,
                   recomm_limit = 3)
```

## Scenario 4

A table of the most popular products.

Comment: If you examine Account Number vs Products for Health vs Asset Mgt, Asset Mgt has 54K records, while Health Insurance has 18K records.

### Dataframe

Get product name and sell date.

```{r}
popular_products <-
as(products_rating_matrix, "data.frame") %>% 
  rename(PRODUCT = item) %>% 
  count(PRODUCT) %>% 
  left_join(product_businesslines, by = "PRODUCT") %>% 
  relocate(BUSINESS_LINE, .after = PRODUCT) %>%
  group_by(BUSINESS_LINE) %>% 
  arrange(desc(n), .by_group = TRUE)

popular_products %>% 
  slice(1:3)
```

Put the results in their own table in the database

```{r}
copy_to(dest = con_zw, df = popular_products, overwrite = TRUE, temporary = FALSE)

dbListTables(conn = con_zw)
```

### Database

Replicate the above results in SQL

```{r}
tbl(src = con_zw, "popular_products") %>%
  group_by(BUSINESS_LINE) %>% 
  collect() %>% 
  slice(1:3)
```

## Scenario 5

Ref: https://trumpexcel.com/dependent-drop-down-list-in-excel/

### Product names

```{r}
#dir.create("../outputs")

tibble(PRODUCTS = colnames(products_rating_matrix)) %>%
  write_csv("../outputs/valid_product_names.csv")
```


```{r}
acc_nums_upload <- 
  readxl::read_excel(
    path = "../../../App/uap_cross_sell/www/files/cs_upload_template.xlsx",
    sheet = "customer_acc") %>% 
  distinct(ACCOUNT_NO) %>% 
  mutate(ACCOUNT_NO = as.character(ACCOUNT_NO)) %>% 
  pull(ACCOUNT_NO)

cust_prods_upload <- 
  readxl::read_excel(
    path = "../../../App/uap_cross_sell/www/files/cs_upload_template.xlsx",
    sheet = "customer_prod") %>% 
  distinct(PRODUCT) %>% 
  pull(PRODUCT)

acc_nums_upload
cust_prods_upload
```

### Customer numbers

Recommendations for customer numbers provided in SQL.

```{r}
predict_on_accounts(account_num = acc_nums_upload, recomm_limit = 3)
```

### Customer products

Recommendations on product list provided.

```{r}
predict_on_choices(user_choices = cust_prods_upload, recomm_limit = 3)
```


# LIMITATIONS

* Product holding is per customer is 1 implying there is no cross-selling happening. This makes it harder to identify cross-selling trends.


# EXPORTS

```{r}
# Project image
save.image(file = "../outputs/02July_Models.RData")
#load(file = "../outputs/24June_Models.RData")

# Objects for deployment
saveRDS(object = zw_rec_model, 
        file = "../../../App/uap_cross_sell/objects/models/Zimbabwe.rds")
saveRDS(object = products_rating_matrix, 
        file = "../../../App/uap_cross_sell/objects/rating_matrix/Zimbabwe.rds")

saveRDS(object = zw_recommendations_detailed,
        file = "../outputs/zw_recommendations_detailed.rds")
```



