---
title: "Develop cross-sell model"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# PACKAGES

```{r message=FALSE, warning=FALSE}
rm(list = ls())

library(doParallel)
library(arrow)
library(sparklyr)
library(dtplyr)
library(recommenderlab)
library(DBI)
library(RSQLite)
library(tidyverse)
```


# VALIDATION

## Evaluation scheme


```{r}
products_rating_matrix <-
readRDS(file = "../../../Data Prep/ZW/02. Create rating matrices/outputs/products_rating_matrix.rds")

dim(products_rating_matrix)
```

Train-test split. Since most customers hold just one product, a cross-validated training cannot be done.

```{r}
summary(rowCounts(products_rating_matrix))
```

```{r}
es <- 
evaluationScheme(data = products_rating_matrix, 
                 method = "split",
                 train = 0.7,
                 k = 1,
                 given = 1)
```

## List of recommenders

```{r}
recommenderRegistry$get_entry_names()
```

AR removed as it does not provide meaningful results despite reducing confidence to 0.0

```{r}
algorithms <- list(
  popular = list(name = "POPULAR", param = NULL),
  ibcf = list(name = "IBCF", param = NULL),
  ubcf = list(name = "UBCF", param = NULL),
  random = list(name = "RANDOM", param = NULL),
  rerecommend = list(name = "RERECOMMEND", param = NULL),
  ar = list(name = "AR", param = list(confidence = 0.0)))
```

## Obtain evaluation results

```{r}
gc()

doParallel::registerDoParallel(parallel::detectCores())

ev_list <- 
evaluate(x = es,
         method = algorithms,
         type = "topNList",
         n = seq(1,10,1))

doParallel::stopImplicitCluster()

gc()
```

# EVALUATION METRICS

## Definition

Reference: [Precision-Recall formula](https://en.wikipedia.org/wiki/Precision_and_recall) in the Information Retrieval context.

Precision: Out of all the recommendations returned by the tool, how many of those were relevant? Having a high number here means that many of the products recommended were relevant to the customer. This can be viewed as a measure of false-positives.

Recall: Out of all the recommendations returned by the tool, were all relevant recommendation in that list or some were missing? Having a high number here means that at least one of those products being recommended was relevant.

## PR Curves

```{r}
names(ev_list)
```


```{r}
plot(x = ev_list, y = "prec/rec", annotate=TRUE, legend="topleft")
```

## Metrics

```{r}
avg(ev_list$ibcf) %>% 
  as_tibble() %>% 
  select(precision,recall,FPR,TPR,n) %>% 
  filter(n %in% c(1,2,3))
```

# MODEL TRAINING

## Train actual model

Notes:
A hybrid recommender will be used since an IBCF recommender on its own does not generate ratings across all business lines.

The hybrid recommender could not be evaluated using the evaluation scheme as the process was taking an extended period of time.

```{r}
zw_ibcf = Recommender(data = products_rating_matrix, method = "IBCF")
zw_pop = Recommender(data = products_rating_matrix, method = "POPULAR")
zw_hyb <- HybridRecommender(zw_ibcf, zw_pop, weights = c(0.9,0.1))


zw_rec_model <- zw_hyb
```


## Extract ratings

### Approach 1

Preferred approach.

```{r eval=FALSE, include=FALSE}
# Chunk taking too long

zw_recommendations <- 
  predict(object = zw_rec_model, 
          newdata = products_rating_matrix, 
          type = "ratings")

```

### Approach 2

Using matrices.

```{r}
zw_ibcf_mat <- 
  predict(object = zw_ibcf,
          newdata = products_rating_matrix,
          type = "ratings") %>% 
  as("matrix")
```


```{r eval=FALSE, include=FALSE}
# Getting a memory error: Run on Google Colab
zw_pop_mat <- 
  predict(object = zw_pop,
          newdata = products_rating_matrix,
          type = "ratings") %>% 
  as("matrix")
```


```{r}
zw_pop_mat <- readRDS(file = "../outputs/zw_pop_mat.rds")

dim(zw_pop_mat)
```


Replace NAs with zero and obtain hybrid matrix.

```{r}
zw_pop_mat[is.na(zw_pop_mat)]=0
zw_ibcf_mat[is.na(zw_ibcf_mat)]=0


zw_hyb_mat = (zw_ibcf_mat*0.9) + (zw_pop_mat*0.1)
```

Obtain first 5 product recommendations of first 2 customers.

```{r}
zw_hyb_mat[1:2,1:5] %>% 
  as.data.frame() %>%
  rownames_to_column("user") %>% 
  pivot_longer(cols = -user,
               names_to = "item",
               values_to = "rating") %>% 
  filter(rating > 0)
```

Comparison of results using `predict()`

```{r}
predict(object = zw_hyb, 
        newdata = products_rating_matrix[1:2,], 
        type = "ratings") %>% 
  as("data.frame") %>% 
  filter(item %in% colnames(products_rating_matrix)[1:5])
```

### Ratings

```{r}
gc()

zw_recommendations <-
zw_hyb_mat %>% 
  as.data.frame() %>% 
  rownames_to_column("UNIQUE_ID") %>%
  pivot_longer(cols = -UNIQUE_ID,
               names_to = "PRODUCT",
               values_to = "rating") %>% 
  filter(rating > 0)

zw_recommendations
```


## Add business lines

Add product business lines to ratings.

```{r}
product_businesslines <-
readRDS(file = "../../../Data Prep/ZW/03. Extract additional customer details/outputs/product_businesslines.rds")

product_businesslines
```


```{r}
zw_recommendations_proc <-
zw_recommendations %>%
  left_join(product_businesslines, by = "PRODUCT")

zw_recommendations_proc
```


```{r eval=FALSE, include=FALSE}
gc()

# copy_to(dest = sc, df = zw_recommendations_proc, overwrite = TRUE, temporary = FALSE, memory = FALSE)

write_parquet(x = zw_recommendations_proc, 
              sink = "../outputs/zw_recommendations_proc.parquet")

```

How many unique ids do we have here?

```{r}
zw_recommendations_proc <-
spark_read_parquet(sc = sc,
                   name = "zw_recommendations_proc",
                   path = "../outputs/parquets/zw_recommendations_proc.parquet")
```


```{r}
proc_unique_ids <- zw_recommendations_proc %>% distinct(UNIQUE_ID)

proc_unique_ids <- proc_unique_ids %>% collect()
```


```{r}
proc_unique_ids
```


## APACHE SPARK

Because we're working with a very large number of records, we will leverage Apache Spark to perform all computations on lazy tables and lazy queries and only execute the final query to a save results in a Postgres database.

Apache Spark with Postgres backend. This has the speed of Apache and persistency of Postgres. Perform all operations on lazy tables and only copy final results to Postgres.


```{r}
config <- spark_config()
config$`sparklyr.shell.driver-class-path` <- "E:\\Softwares\\postgresql-42.2.22.jar"
```


```{r}
try(expr = spark_disconnect(sc = sc))

sc <- spark_connect(master = "local", config = config)

sc
```


## Add max rating, arrange

* Add maximum product rating per customer
* Sort by max rating and rating per customer
* Group data by customer and business line
* Limit to top 5 recommendations per customer per business line


```{r eval=FALSE, include=FALSE}
zw_recommendations_df <-
zw_recommendations_proc %>%
  group_by(UNIQUE_ID) %>% 
  mutate(max_rating = max(rating)) %>% 
  arrange(desc(max_rating),
          desc(rating), .by_group = TRUE) %>% 
  ungroup() %>%
  group_by(UNIQUE_ID, BUSINESS_LINE) %>% 
  slice(1:5)
```


Read parquet into Hadoop.

```{r}
gc()

registerDoParallel(cores = 2)

zw_recommendations_proc <- 
  spark_read_parquet(sc = sc,
                     path = "../outputs/parquets/zw_recommendations_proc.parquet",
                     name = "zw_recommendations_proc")

class(zw_recommendations_proc)
```

Replicate the above in SQL. Because `slice()` is not implemented yet, well design a workaround in SQL using `RANK()`:
* Group the dataframe as required
* Add a rank column
* Filter top 5 by rank

Perform SQL operations on the lazy table.

```{r}
zw_recommendations_df_query <-
  zw_recommendations_proc %>% 
  group_by(UNIQUE_ID) %>% 
  mutate(max_rating = max(rating)) %>% 
  arrange(desc(max_rating),
          desc(rating), .by_group = TRUE) %>% 
  ungroup() %>%
  group_by(UNIQUE_ID, BUSINESS_LINE) %>%
  mutate(rank = row_number()) %>% 
  filter(rank <= 5)

class(zw_recommendations_df_query)
```


Perform all operations on lazy tables and only copy the final results to Postgres.

Write table from Hadoop into Postgres.

Useful links:
* https://github.com/sparklyr/sparklyr/issues/1067
* https://therinspark.com/data.html#jdbc

```{r}
gc()

registerDoParallel(cores = 2)

spark_write_jdbc(x = zw_recommendations_df_query, 
                 name = "zw_recommendations_df", 
                 mode = "overwrite",
                 options = list(url = "jdbc:postgresql://localhost:5432/zw_dataprep",
                                user = "postgres",
                                password = "john"))

stopImplicitCluster()
```


Read table from Postgres to Hadoop.


```{r eval=FALSE, include=FALSE}
# Not used. Taking too long. A parquet file was used instead.

registerDoParallel(cores = 2)

spark_read_jdbc(sc = sc, 
                name = "zw_recommendations_df", 
                options = list(url = "jdbc:postgresql://localhost:5432/zw_dataprep",
                               user = "postgres",
                               password = "john",
                               dbtable = "zw_recommendations_df"))

stopImplicitCluster()
```


Write parquet file.

```{r}
registerDoParallel(cores = 2)

zw_recommendations_df <- 
  tbl(src = con_prep, "zw_recommendations_df") %>% 
  collect()

stopImplicitCluster()
```


```{r}
write_parquet(x = zw_recommendations_df,
              sink = "../outputs/parquets/zw_recommendations_df.parquet")
```


## Add product values

Replace local dataframes with lazy Postgres tables.


```{r}
zw_recommendations_df <- tbl(src = sc, "zw_recommendations_df")

class(zw_recommendations_df)
```

```{r}
product_values <- readRDS(file = "../../../Data Prep/ZW/03. Extract additional customer details/outputs/product_values.rds")

product_values <- 
  copy_to(dest = sc,
          df = product_values,
          name = "product_values",
          overwrite = TRUE,
          temporary = FALSE)

class(product_values)
```


```{r}
zw_recommendations_df_proc <-
zw_recommendations_df %>% 
  left_join(product_values, by = "PRODUCT")

class(zw_recommendations_df_proc)
```

Execute results to remote database. This is faster than `copy_to()`

```{r}
compute(x = zw_recommendations_df_proc, 
        name = "zw_recommendations_df_proc", 
        #temporary = FALSE, 
        overwrite = TRUE)
```


```{r}
zw_recommendations_df_proc %>% 
  anyNA()
```


## Add Accounts

Add all Account Numbers associated with each Unique ID


```{r}
holding_paths <-
dir(path = "../../../Data Prep/ZW/01. Determine product holding/outputs/objects/",
    pattern = "holding",
    all.files = FALSE, 
    full.names = TRUE, 
    ignore.case = TRUE)

uniqueid_accounts <-
holding_paths %>% 
  map_dfr(.f = readRDS) %>% 
  select(UNIQUE_ID, ACCOUNT_NO) %>% 
  distinct(UNIQUE_ID, .keep_all = TRUE)
```

Convert to lazy query.

```{r}
uniqueid_accounts <-
  copy_to(dest = sc,
          df = uniqueid_accounts,
          name = "uniqueid_accounts",
          overwrite = TRUE,
          temporary = FALSE)

class(uniqueid_accounts)
```


```{r}
zw_recommendations_unique_id <-
zw_recommendations_df_proc %>%
  left_join(uniqueid_accounts, by = "UNIQUE_ID") %>%
  relocate(ACCOUNT_NO, .after = UNIQUE_ID) %>% 
  ungroup() %>%
  group_by(ACCOUNT_NO, BUSINESS_LINE)

class(zw_recommendations_unique_id)
```


```{r}
compute(x = zw_recommendations_unique_id, 
        name = "zw_recommendations_unique_id",
        overwrite = TRUE)
```


```{r}
tbl(src = sc, "zw_recommendations_unique_id") %>% 
  anyNA()
```

## POSTGRES

This is a database created to analyse and store data preparation results. Spark has challenges handling glued columns.


## Add customer details & grouping


```{r}
con_prep <- dbConnect(drv = RPostgres::Postgres(), 
                      user = "postgres", 
                      password = "john", 
                      dbname = "zw_dataprep")

dbListTables(conn = con_prep)
```

Merge ratings with customer details.

```{r}
customer_details_final <-
readRDS(file = "../../../Data Prep/ZW/03. Extract additional customer details/outputs/customer_details_final.rds")

customer_details_final <-
  copy_to(dest = con_prep,
          df = customer_details_final,
          name = "customer_details_final",
          temporary = FALSE,
          overwrite = TRUE)

class(customer_details_final)
```

Move zw_recommendations_unique_id to Postgres database.

```{r}
class(zw_recommendations_unique_id)
class(customer_details_final)
```


```{r}
spark_write_jdbc(x = zw_recommendations_unique_id, 
                 name = "zw_recommendations_unique_id", 
                 mode = "overwrite",
                 options = list(url = "jdbc:postgresql://localhost:5432/zw_dataprep",
                                user = "postgres",
                                password = "john"))

dbListTables(conn = con_prep)
```

Create Postgre lazy query

```{r}
zw_recommendations_unique_id <- tbl(src = con_prep, "zw_recommendations_unique_id")
```


```{r}
class(zw_recommendations_unique_id)
class(customer_details_final)
```


```{r}
zw_recommendations_detailed <-
zw_recommendations_unique_id %>% 
  left_join(customer_details_final, by = "UNIQUE_ID") %>% 
  relocate(BUSINESS_LINE, .after = PRODUCT) %>% 
  group_by(UNIQUE_ID, BUSINESS_LINE)

class(zw_recommendations_detailed)
```

Send results to remote database.

```{r}
try(dbRemoveTable(conn = con_prep, name = "zw_recommendations_detailed"))

compute(x = zw_recommendations_detailed,
        name = "zw_recommendations_detailed",
        temporary = FALSE)
```

## Sense check

Check if all 715K customers are in the database. Checking also for NA values.

```{r}
rec_accounts <- 
  tbl(src = con_prep, "zw_recommendations_detailed") %>%
  distinct(UNIQUE_ID, ACCOUNT_NO)

class(rec_accounts)
```


```{r}
try(dbRemoveTable(conn = con_prep, name = "UNIQUE_IDs"))

compute(x = rec_accounts,
        name = "rec_accounts",
        temporary = FALSE)
```


```{r}
rec_accounts <-
  tbl(src = con_prep, "rec_accounts") %>% 
  collect()
```

If we exclude NA accounts, we get our original 715K. Are these UNIQUE_IDs with NAs already assigned account numbers?

```{r}
rec_accounts %>% 
  filter(is.na(ACCOUNT_NO))


rec_accounts %>% 
  filter(!is.na(ACCOUNT_NO)) %>% 
  dim()
```

```{r}
# without account no
rec_accounts %>% 
  filter(is.na(ACCOUNT_NO))
  pull(UNIQUE_ID)


# with account no
rec_accounts %>% 
  filter(!is.na(ACCOUNT_NO)) %>% 
  pull(UNIQUE_ID)


length(
intersect(x = rec_accounts %>% 
              filter(is.na(ACCOUNT_NO)) %>% 
              pull(UNIQUE_ID),
          
          y = rec_accounts %>% 
              filter(!is.na(ACCOUNT_NO)) %>% 
              pull(UNIQUE_ID))
)
```

```{r}
zw_recommendations_detailed <- tbl(src = con_prep, "zw_recommendations_detailed")
```

Filter out NA Account Numbers.

```{r}
recommendations_detailed <-
zw_recommendations_detailed %>% 
  filter(!is.na(ACCOUNT_NO))

class(recommendations_detailed)
```


```{r}
recommendations_detailed_df <-
  recommendations_detailed %>% 
  collect()

length(unique(recommendations_detailed_df$UNIQUE_ID))
```


```{r}
table_order <- c("ACCOUNT_NO", "PRODUCT", "BUSINESS_LINE", "rating", "max_rating", "ownership", "intermediated", "product_value", "customer_details")

zw_recommendations_detailed <-
recommendations_detailed_df %>% 
  select(all_of(table_order))
```


Write parquet file.

```{r}
write_parquet(x = zw_recommendations_detailed,
              sink = "../outputs/parquets/zw_recommendations_detailed.parquet")
```


# CREATE DATABASE

Create sqlite3 database to hold and query records. One country, one database, many tables.

Ref: https://www.youtube.com/watch?v=wXEZZ2JT3-k

## Database Connection

```{r}
zw_sqlite_path <- "../../../App/uap_cross_sell/db/zw_cs.db"
con_zw <- dbConnect(drv = SQLite(), zw_sqlite_path)
```


```{r}
dbListTables(conn = con_zw)
```

## Database tables

Create database tables of required data.

### Detailed recommendations

Arrange the table.

```{r}
zw_recommendations_detailed <-
zw_recommendations_detailed %>% 
  arrange(desc(max_rating), 
          ACCOUNT_NO, 
          BUSINESS_LINE, 
          desc(rating))
```


```{r}
copy_to(dest = con_zw,
        df = zw_recommendations_detailed,
        name = "recommendations_detailed",
        overwrite = TRUE,
        temporary = FALSE)
```


```{r}
tbl(src = con_zw, "recommendations_detailed")
```


Compare the two tables...

```{r}
dim(zw_recommendations_detailed)

tbl(src = con_zw, "recommendations_detailed") %>% 
  collect() %>% 
  dim()
```


### Product business lines

```{r}
copy_to(dest = con_zw, 
        df = product_businesslines, 
        name = "product_businesslines", 
        overwrite = TRUE, 
        temporary = FALSE)
```


### Product values

```{r}
copy_to(dest = con_zw, 
        df = product_values, 
        name = "product_values", 
        overwrite = TRUE,
        temporary = FALSE)
```


## Confirm tables

```{r}
dbListTables(conn = con_zw)
```


```{r}
tbl(src = con_zw, "recommendations_detailed")
```


```{r eval=FALSE, include=FALSE}
# Not required
q <- "UPDATE product_values SET product_value = 99999, accounts = 99999"

DBI::dbSendQuery(conn = con_zw, statement = q)
```


# PREDICTIONS

## Scenario 1

Where you have a customer account number and want to 3 products per business line.

We select a customer whose unique identifier is:
* Account Number: 981741
* Hashed National ID: c11be68eb394fac45c32076299d16a386b771318675d348abcf8d76866f97866
* Multiple Account Numbers: f09b93a542666d06d857fab8ccc0e2a7ca8697c1b4ae706aa0e717cd49383a66

```{r}
#sample_cust_num <- paste0(rownames(products_rating_matrix)[1:2], collapse =",")
#records <- strsplit(x = sample_cust_num, split = ",")[[1]]

unique_ids <- c("981741",
                "c11be68eb394fac45c32076299d16a386b771318675d348abcf8d76866f97866",
                "f09b93a542666d06d857fab8ccc0e2a7ca8697c1b4ae706aa0e717cd49383a66")
```

### Predict

```{r}
predict(object = zw_rec_model, 
        newdata = products_rating_matrix[unique_ids,], 
        type = "ratings") %>% 
  as("data.frame") %>% 
  left_join(product_businesslines, by = c("item"="PRODUCT")) %>% 
  group_by(user, BUSINESS_LINE) %>% 
  arrange(desc(rating)) %>% 
  slice(1:3)
```

### Dataframe

Test cases
* A customer with multiple accounts ["10039100,30022583,41398995,30105335"].
* Two customers with multiple accounts ["41505354,41505340,41505336].
* A customer with multiple accounts and another without multiple accounts ["981741"].

#### Approach 1

```{r}
# Too slow
zw_recommendations_detailed %>% 
  filter(str_detect(string = ACCOUNT_NO, pattern = "30105335")) %>% 
  group_by(ACCOUNT_NO, BUSINESS_LINE) %>% 
  slice(1:3)
```

#### Approach 2

```{r}
convert_nums_to_pattern <- function(account_num){
  
  account_num_vec <- strsplit(x = account_num, split = ",")[[1]]
  pattern_account_num <- paste0(account_num_vec, collapse = "|")
  return(pattern_account_num)
  
}
```

Test the function.

```{r}
input_account_num <- str_remove_all(string = "30105335,41505340,  981741",
                                     pattern = " ")

convert_nums_to_pattern(account_num = input_account_num)
```
Implement the function.

```{r}
# Significantly faster
input_account_num <- c("30105335,   41505340,981741")
input_account_num <- str_remove_all(string = input_account_num, 
                                    pattern = " ")

cust_unique_ids <-
uniqueid_accounts %>% 
  filter(str_detect(string = ACCOUNT_NO, 
                    pattern = convert_nums_to_pattern(
                      account_num = input_account_num)
                    )) %>% 
  pull(UNIQUE_ID)


predict(object = zw_rec_model, 
        newdata = products_rating_matrix[cust_unique_ids,], 
        type = "ratings") %>% 
  as("data.frame") %>% 
  rename(UNIQUE_ID = user, PRODUCT = item) %>% 
  left_join(product_businesslines, by = "PRODUCT") %>% 
  left_join(uniqueid_accounts, by = "UNIQUE_ID") %>% 
  select(any_of(c("ACCOUNT_NO",table_order))) %>% 
  select(-UNIQUE_ID) %>% 
  group_by(ACCOUNT_NO, BUSINESS_LINE) %>% 
  arrange(desc(rating)) %>% 
  slice(1:3)
```

### Database

Database replication of the above results.

Save `uniqueid_accounts` to database

```{r}
copy_to(dest = con_zw,
        df = uniqueid_accounts,
        name = "uniqueid_accounts",
        overwrite = TRUE,
        temporary = FALSE)
```

#### REGEX

Regex in SQL.

```{r message=FALSE, warning=FALSE}
# Requires loading a udf from a dll file.
# NOT USED
query_stmt <- "SELECT * FROM uniqueid_accounts WHERE ACCOUNT_NO REGEXP '30105335';"
query_res <- dbSendStatement(conn = con_zw, statement = query_stmt)
```

#### LIKE

Using LIKE operator.

```{r}
query_stmt <- "select * from uniqueid_accounts limit 6;"
query_result <- dbSendStatement(conn = con_zw, statement = query_stmt)

query_stmt1 <- "SELECT * FROM uniqueid_accounts WHERE ACCOUNT_NO like '%30105335%';"
query_stmt2 <- "SELECT * FROM uniqueid_accounts WHERE ACCOUNT_NO like '%41505340%';"
query_stmt3 <- "SELECT * FROM uniqueid_accounts WHERE ACCOUNT_NO like '%981741%';"

query_result1 <- dbSendStatement(conn = con_zw, statement = query_stmt1)
dbFetch(res = query_result1)
dbClearResult(res = query_result1)

query_result2 <- dbSendStatement(conn = con_zw, statement = query_stmt2)
dbFetch(res = query_result2)
dbClearResult(res = query_result2)

query_result3 <- dbSendStatement(conn = con_zw, statement = query_stmt3)
dbFetch(res = query_result3)
dbClearResult(res = query_result3)
```

Applying `glue`

```{r}
test_acc <- "30105335"

test_stmt <-
  paste0("SELECT * FROM uniqueid_accounts WHERE ACCOUNT_NO LIKE '%",test_acc,"%'") %>% 
  SQL()

test_res <- dbSendQuery(conn = con_zw, statement = test_stmt)
test_tbl <- dbFetch(test_res)
dbClearResult(res = test_res)
test_tbl
```

Adding a `for()` loop

```{r}
input_account_num <- c("30105335,   41505340,981741")
input_account_num <- str_remove_all(string = input_account_num, 
                                    pattern = " ")
input_account_num <- str_split(string = input_account_num, 
                               pattern = ",")[[1]]


res_tbl <- tibble()
  
for(account in seq_along(input_account_num)){
  
  query_stmt <-
  paste0("SELECT * FROM uniqueid_accounts WHERE ACCOUNT_NO LIKE '%",input_account_num[account],"%'") %>% 
  SQL()
  
  query_result <- 
  dbSendStatement(conn = con_zw, 
                  statement = query_stmt)
  
  res_tbl <- rbind(res_tbl, dbFetch(res = query_result))
  dbClearResult(res = query_result)
}

cust_unique_ids <- res_tbl %>% pull(UNIQUE_ID)

cust_unique_ids
```

#### Predict

```{r}
predict(object = zw_rec_model, 
        newdata = products_rating_matrix[cust_unique_ids,], 
        type = "ratings") %>% 
  as("data.frame") %>% 
  rename(UNIQUE_ID = user, PRODUCT = item) %>% 
  left_join(product_businesslines, by = "PRODUCT") %>% 
  left_join(uniqueid_accounts, by = "UNIQUE_ID") %>% 
  left_join(product_values, by = "PRODUCT") %>% 
  left_join(customer_details_final, by = "UNIQUE_ID") %>% 
  select(any_of(c("ACCOUNT_NO",table_order))) %>% 
  select(-UNIQUE_ID) %>% 
  group_by(ACCOUNT_NO, BUSINESS_LINE) %>% 
  mutate(max_rating = max(rating)) %>% 
  relocate(max_rating, .after = rating) %>% 
  arrange(desc(rating)) %>% 
  slice(1:3)
```


```{r}
copy_to(dest = con_zw, 
        df = customer_details_final, 
        name = "customer_details_final",
        overwrite = TRUE, 
        temporary = FALSE)
```


#### User defined function


```{r}
predict_on_accounts <- function(account_num, recomm_limit){
  
# Clean & vectorize input values
input_account_num <- str_remove_all(string = account_num, 
                                    pattern = " ")
input_account_num <- str_split(string = input_account_num, 
                               pattern = ",")[[1]]

# Retrieve unique identifiers
res_tbl <- tibble()
  
for(account in seq_along(input_account_num)){
  
  query_stmt <-
  paste0("SELECT * FROM uniqueid_accounts WHERE ACCOUNT_NO LIKE '%",input_account_num[account],"%'") %>% 
  SQL()
  
  query_result <- 
  dbSendStatement(conn = con_zw, 
                  statement = query_stmt)
  
  res_tbl <- rbind(res_tbl, dbFetch(res = query_result))
  dbClearResult(res = query_result)
}

cust_unique_ids <- res_tbl %>% pull(UNIQUE_ID)

# Predict & merge
user_account_recommendations <-
predict(object = zw_rec_model, 
        newdata = products_rating_matrix[cust_unique_ids,], 
        type = "ratings") %>% 
  as("data.frame") %>% 
  rename(UNIQUE_ID = user, PRODUCT = item) %>% 
  left_join(product_businesslines, by = "PRODUCT") %>% 
  left_join(uniqueid_accounts, by = "UNIQUE_ID") %>% 
  left_join(product_values, by = "PRODUCT") %>% 
  left_join(customer_details_final, by = "UNIQUE_ID") %>% 
  select(any_of(c("ACCOUNT_NO",table_order))) %>% 
  select(-UNIQUE_ID) %>% 
  group_by(ACCOUNT_NO, BUSINESS_LINE) %>% 
  mutate(max_rating = max(rating)) %>% 
  relocate(max_rating, .after = rating) %>% 
  arrange(desc(rating)) %>% 
  slice(1:recomm_limit)

return(user_account_recommendations)
}
```

Test the function.

```{r}
predict_on_accounts(account_num = "30105335,   41505340,981741",
                    recomm_limit = 3)
```


## Scenario 2

You want to sort all customers across the group according to who's most likely to make a purchase and return 3 recommendations per business line.

Due to performance challenges, only the top 1000 records will be displayed while remaining dataset can be availed as a link for downloading.

We allow the end-user get the top n recommended customers based on this criteria:
* Intermediated - Yes/No
* Ownership - business line that owns the customer
* Product value - a range
* Rank e.g. 1-10

### Dataframe

```{r}
zw_recommendations_detailed %>% 
  ungroup() %>% 
  count(ownership)

zw_recommendations_detailed %>% 
  ungroup() %>% 
  filter(intermediated=="No") %>% 
  count(ownership)
```

```{r}
zw_recommendations_detailed %>% 
  filter(intermediated=="No") %>% 
  filter(ownership=="Lending") %>% 
  filter(product_value>=50000) %>% 
  filter(product_value<=200000) %>% 
  slice(1:3)
 #head(1000)
```

### Database

```{r}
tbl(src = con_zw, "recommendations_detailed") %>%
  group_by(ACCOUNT_NO, BUSINESS_LINE) %>% 
  filter(intermediated=="No") %>% 
  filter(ownership=="Lending") %>% 
  filter(product_value >= 50000) %>% 
  filter(product_value <= 200000) %>% 
  collect() %>% 
  slice(1:3)
  #head(1000)
```


## Scenario 3

Include additional products on top of what one has purchased and return 3 recommendations per business line.

### Procedure

Capture user selections. One product from each business line.

```{r}
user_choices <-
product_businesslines %>%
  group_by(BUSINESS_LINE) %>%
  slice_sample(n = 1) %>%
  pull(PRODUCT)

user_choices
```

Identify missing product names

```{r}
not_user_choices <- colnames(products_rating_matrix)[!colnames(products_rating_matrix) %in% user_choices]

length(not_user_choices)
```

Convert to dataframe.

```{r}
choices <- tibble(choice = 
rep(x = c(1,0), 
    times = c(length(user_choices), 
              length(not_user_choices))
    ))

user_choices_df <-
choices %>% 
  mutate(user_choices = c(user_choices, not_user_choices))

user_choices_df
```


```{r}
user_choices_df <- 
pivot_wider(data = user_choices_df,
            names_from = user_choices, 
            values_from = choice)

user_choices_df
```

Convert to binary rating matrix.

```{r}
user_choices_mat <- as.matrix(user_choices_df)

colnames(user_choices_mat)[1:10]

user_choices_ratmat <- as(user_choices_mat,"binaryRatingMatrix")

class(user_choices_ratmat)
```

Recommend products.

```{r}
user_choice_recommendations <- 
  predict(object = zw_rec_model, 
          newdata = user_choices_ratmat, 
          type="ratings")
```


```{r}
as(user_choice_recommendations, "data.frame") %>% 
  rename(PRODUCT = item) %>% 
  left_join(product_businesslines, by = "PRODUCT") %>% 
  left_join(product_values, by = "PRODUCT") %>% 
  relocate(BUSINESS_LINE, .after = PRODUCT) %>% 
  group_by(BUSINESS_LINE) %>% 
  arrange(desc(rating), .by_group = TRUE) %>%
  select(-accounts) %>% 
  slice(1:3)
```

### User-defined function

Convert to a user function

```{r}
predict_on_choices <- function(user_choices, recomm_limit){
  
  #Products not selected
  not_user_choices <- colnames(products_rating_matrix)[!colnames(products_rating_matrix) %in% user_choices]
  
  #Convert to a table
  choices <- 
    tibble(choice = rep(x = c(1,0), 
                        times = c(length(user_choices), 
                                  length(not_user_choices))
    ))

user_choices_df <-
choices %>% 
  mutate(user_choices = c(user_choices, not_user_choices))
  
 #Convert to wide format
user_choices_df <- 
pivot_wider(data = user_choices_df,
            names_from = user_choices, 
            values_from = choice)

 #Convert to binaryRatingMatrix
user_choices_mat <- as.matrix(user_choices_df)
user_choices_ratmat <- as(user_choices_mat,"binaryRatingMatrix")

 #Predict using model
user_choice_recommendations <- 
  predict(object = zw_rec_model, 
          newdata = user_choices_ratmat, 
          type="ratings")

 #Convert prediction to dataframe
user_choice_recommendations_df <-
as(user_choice_recommendations, "data.frame") %>% 
  rename(PRODUCT = item) %>%
  left_join(product_businesslines, by = "PRODUCT") %>% 
  left_join(product_values, by = "PRODUCT") %>% 
  relocate(BUSINESS_LINE, .after = PRODUCT) %>% 
  group_by(BUSINESS_LINE) %>% 
  arrange(desc(rating), .by_group = TRUE) %>% 
  slice(1:recomm_limit) %>% 
  select(-accounts)

return(user_choice_recommendations_df)
}
```


```{r}
predict_on_choices(user_choices = user_choices,
                   recomm_limit = 3)
```

## Scenario 4

A table of the most popular products.

Comment: If you examine Account Number vs Products for Health vs Asset Mgt, Asset Mgt has 54K records, while Health Insurance has 18K records.

### Dataframe

Get product name and sell date.

```{r}
popular_products <-
as(products_rating_matrix, "data.frame") %>% 
  rename(PRODUCT = item) %>% 
  count(PRODUCT) %>% 
  left_join(product_businesslines, by = "PRODUCT") %>% 
  relocate(BUSINESS_LINE, .after = PRODUCT) %>%
  group_by(BUSINESS_LINE) %>% 
  arrange(desc(n), .by_group = TRUE)

popular_products %>% 
  slice(1:3)
```

Put the results in their own table in the database

```{r}
copy_to(dest = con_zw, df = popular_products, overwrite = TRUE, temporary = FALSE)

dbListTables(conn = con_zw)
```

### Database

Replicate the above results in SQL

```{r}
tbl(src = con_zw, "popular_products") %>%
  group_by(BUSINESS_LINE) %>% 
  collect() %>% 
  slice(1:3)
```

## Scenario 5

Ref: https://trumpexcel.com/dependent-drop-down-list-in-excel/

### Product names

```{r}
#dir.create("../outputs")

tibble(PRODUCTS = colnames(products_rating_matrix)) %>%
  write_csv("../outputs/valid_product_names.csv")
```


```{r}
acc_nums_upload <- 
  readxl::read_excel(
    path = "../../../App/uap_cross_sell/www/files/cs_upload_template.xlsx",
    sheet = "customer_acc") %>% 
  distinct(ACCOUNT_NO) %>% 
  mutate(ACCOUNT_NO = as.character(ACCOUNT_NO)) %>% 
  pull(ACCOUNT_NO)

cust_prods_upload <- 
  readxl::read_excel(
    path = "../../../App/uap_cross_sell/www/files/cs_upload_template.xlsx",
    sheet = "customer_prod") %>% 
  distinct(PRODUCT) %>% 
  pull(PRODUCT)

acc_nums_upload
cust_prods_upload
```

### Customer numbers

Recommendations for customer numbers provided in SQL.

```{r}
predict_on_accounts(account_num = acc_nums_upload, recomm_limit = 3)
```

### Customer products

Recommendations on product list provided.

```{r}
predict_on_choices(user_choices = cust_prods_upload, recomm_limit = 3)
```


# LIMITATIONS

* Product holding is per customer is 1 implying there is no cross-selling happening. This makes it harder to identify cross-selling trends.


# EXPORTS

```{r}
# Project image
save.image(file = "../outputs/02July_Models.RData")
load(file = "../outputs/24June_Models.RData")

# Objects for deployment
saveRDS(object = zw_rec_model, 
        file = "../../../App/uap_cross_sell/objects/models/Zimbabwe.rds")
saveRDS(object = products_rating_matrix, 
        file = "../../../App/uap_cross_sell/objects/rating_matrix/Zimbabwe.rds")
saveRDS(object = zw_recommendations_detailed,
        file = "../outputs/zw_recommendations_detailed.rds")
```


```{r}
dim(products_rating_matrix)

zw_rec_model
```


